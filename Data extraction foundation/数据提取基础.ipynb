{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1：   Data extraction fundamentals(数据提取基础)\n",
    "\n",
    "## 简介\n",
    "\n",
    "Data --> Gathering --> Extracting --> cleaning --> Storing --> Analysis \n",
    "\n",
    "We need to asses out data to\n",
    " - TEST ASSUMPTION ABOUT\n",
    "  - VALUES\n",
    "  - DATA TYPE\n",
    "  - SHAPE\n",
    " - IDENTITFY ERRORS OR OUTLINERS(异常值)\n",
    " - FIND MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> 数据科学家往往关心数据包含哪些项（what items），以及这些项有哪些字段\n",
    "\n",
    "***常见数据格式***：\n",
    "- 表格\n",
    "- CSV （TSV是以制表符为分隔符）\n",
    "- JSON\n",
    "- HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prictice --> Parsing CSV Files\n",
    "import os\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "DATADIR = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    with open(datafile, \"r\") as f:       \n",
    "        header = f.readline().split(',')\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if count == 10:\n",
    "                break\n",
    "            \n",
    "            fields = line.split(',')\n",
    "            entry = {}\n",
    "            \n",
    "            for i,value in enumerate(fields):\n",
    "                entry[header[i].strip()] = value.strip()\n",
    "            \n",
    "            count +=1\n",
    "            data.append(entry)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    \n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Using CSV module\n",
    "\n",
    "Python在处理CSV文件时，调用CSV模块，他能为你处理任何CSV能遇到的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BPI Certification': '',\n",
       " 'Label': 'Parlophone(NZ), Capitol(US)',\n",
       " 'RIAA Certification': 'Platinum',\n",
       " 'Released': '14 June 1965',\n",
       " 'Title': 'Beatles VI',\n",
       " 'UK Chart Position': '-',\n",
       " 'US Chart Position': '1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    with open(datafile,'rb') as sd:\n",
    "        r = csv.DictReader(sd)\n",
    "        \n",
    "        for line in r:\n",
    "            data.append(line)\n",
    "    return data \n",
    "\n",
    "\n",
    "datafile = os.path.join(DATADIR, DATAFILE)           \n",
    "parse_csv(datafile)[13]           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 10. Introduce to xlrd\n",
    "***Help on package xlrd***:   \n",
    "NAME  \n",
    "* xlrd  \n",
    "\n",
    "PACKAGE CONTENTS  \n",
    "* biffh  \n",
    "* book  \n",
    "* compdoc  \n",
    "* formatting  \n",
    "* formula  \n",
    "* info  \n",
    "* licences  \n",
    "* sheet\n",
    "  * Cell    \n",
    "  * Colinfo\n",
    "  * Hyperlink\n",
    "  * MSODrawing\n",
    "  * MSObj\n",
    "  * MSTxo\n",
    "  * Note\n",
    "  * Rowinfo\n",
    "  * Sheet\n",
    "    * cell(self, rowx, colx)\n",
    "    * cell_type(self, rowx, colx)\n",
    "    * cell_value(self, rowx, colx)\n",
    "    * cell_xf_index(self, rowx, colx)\n",
    "    * col = col_slice(self, colx, start_rowx=0, end_rowx=None)\n",
    "    * col_slice(self, colx, start_rowx=0, end_rowx=None)\n",
    "    * col_types(self, colx, start_rowx=0, end_rowx=None)\n",
    "    * col_values(self, colx, start_rowx=0, end_rowx=None)\n",
    "    * computed_column_width(self, colx)\n",
    "    * row(self, rowx)\n",
    "    * row_len(self, rowx)\n",
    "    * row_slice(self, rowx, start_colx=0, end_colx=None)\n",
    "    * row_types(self, rowx, start_colx=0, end_colx=None)\n",
    "    * row_values(self, rowx, start_colx=0, end_colx=None)\n",
    "* timemachine  \n",
    "* xldate  \n",
    "* xlsx  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.088697\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9238.73731 1438.20528 1565.442856 916.708348 14010.903488 3027.98334 6165.211119 1157.741663 37520.933404 \n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.088697\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "DATES:\n",
      "Type of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xlrd\n",
    "\n",
    "DATADIR = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/\"\n",
    "filename = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "datafile = os.path.join(DATADIR,filename)\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "\n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\",\n",
    "    print data[3][2]\n",
    "\n",
    "    print \"\\nCells in a nested loop:\"    \n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print sheet.cell_value(row, col),\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\", \n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3, 2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3, 2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    print \"\\nDATES:\"\n",
    "    print \"Type of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1, 0)\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print \"Time in Excel format:\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Quiz: Reading Excel File\n",
    "\n",
    "主要是针对单元格，行，列的操作 练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import xlrd\n",
    "\n",
    "DATADIR = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/\"\n",
    "filename = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "datafile = os.path.join(DATADIR,filename)\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "    \n",
    "    \n",
    "    cv = sheet.col_values(1,start_rowx=1,end_rowx=None)\n",
    "    \n",
    "    maxvalue = max(cv)\n",
    "    minvalue = min(cv)\n",
    "\n",
    "    maxpos = cv.index(maxvalue) + 1\n",
    "    minpos = cv.index(minvalue) + 1\n",
    "    \n",
    "    maxtime = sheet.cell_value(maxpos,0)\n",
    "    max_real_time = xlrd.xldate_as_tuple(maxtime,0)\n",
    "    mintime = sheet.cell_value(minpos,0)\n",
    "    min_real_time = xlrd.xldate_as_tuple(mintime,0)\n",
    "    \n",
    "    col_len = float(len(cv))\n",
    "    \n",
    "    avgcoast = sum(cv)/col_len\n",
    "\n",
    "    \n",
    "    data = {\n",
    "            'maxtime': max_real_time,\n",
    "            'maxvalue': maxvalue,\n",
    "            'mintime': min_real_time,\n",
    "            'minvalue': minvalue,\n",
    "            'avgcoast': avgcoast\n",
    "    }\n",
    "    return data  \n",
    "\n",
    "\n",
    "def test():\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Quiz: JSSON playgroud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://musicbrainz.org/ws/2/artist/\n",
      "{'query': 'artist:Nirvana'}\n",
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\", \n",
      "                \"name\": \"Finland\", \n",
      "                \"sort-name\": \"Finland\"\n",
      "            }, \n",
      "            \"country\": \"FI\", \n",
      "            \"disambiguation\": \"Early 1980's Finnish punk band\", \n",
      "            \"id\": \"85af0709-95db-4fbc-801a-120e9f4766d0\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"finland\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"founded in 1987 by a Michael Jackson double/imitator\", \n",
      "            \"id\": \"3aa878c0-224b-41e5-abd1-63be359d2bca\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1987\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"French band from Martigues, activ during the 70s.\", \n",
      "            \"id\": \"c49d69dc-e008-47cf-b5ff-160fafb1fe1f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"60s band from the UK\", \n",
      "            \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1967\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"progressive rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"orchestral\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"british\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"power pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"psychedelic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"soft rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"symphonic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"english\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Nirvana US\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Nirvana US\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"a640b45c-c173-49b1-8030-973603e895b5\", \n",
      "                \"name\": \"Aberdeen\", \n",
      "                \"sort-name\": \"Aberdeen\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"90s US grunge band\", \n",
      "            \"id\": \"5b11f4ce-a62d-471e-81fc-a69a8278c7da\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988-01\", \n",
      "                \"end\": \"1994-04-05\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Nirvana\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"Nirvana\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 9, \n",
      "                    \"name\": \"rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 4, \n",
      "                    \"name\": \"alternative rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"90s\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 5, \n",
      "                    \"name\": \"american\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"seattle\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 14, \n",
      "                    \"name\": \"grunge\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"band\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"usa\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"alternative\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"am\\u00e9ricain\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"legendary\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"acoustic rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"noise rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"90\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"northwest\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"rock and indie\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"united states\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"nirvana\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 0, \n",
      "                    \"name\": \"kurt cobain\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"id\": \"c3a64a25-251b-4d03-afba-1471440245b8\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Approaching Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Approaching Nirvana\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"b305320e-c158-43f4-b5be-4450e2f99a32\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"El Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana, El\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Nirvana\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Nirvana\", \n",
      "                    \"type\": null\n",
      "                }, \n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Prophet 2002\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Prophet 2002\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\", \n",
      "                \"name\": \"Sweden\", \n",
      "                \"sort-name\": \"Sweden\"\n",
      "            }, \n",
      "            \"country\": \"SE\", \n",
      "            \"disambiguation\": \"Swedish death metal band\", \n",
      "            \"id\": \"f2dfdff9-3862-4be0-bf85-9c833fa3059e\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana 2002\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana 2002\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"329c04ae-3b73-4ca3-996f-75608ab1befb\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Singh\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Singh, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"gender\": \"female\", \n",
      "            \"id\": \"206419e0-3a7a-49ce-8437-4e757767d02b\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Savoury\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Savoury, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"86f9ae24-ba2a-4d55-9275-0b89b85f6e3a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Weed Nirvana\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Weed Nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"07607044-8140-47ba-bb24-7129babe586b\", \n",
      "                \"name\": \"West Midlands\", \n",
      "                \"sort-name\": \"West Midlands\"\n",
      "            }, \n",
      "            \"id\": \"8f32371e-4bac-4090-ba13-2c1cd1aeab0d\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana UK\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Nirvana UK\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"f58febd3-18de-4371-8d95-4a68d4f79456\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Kelly\", \n",
      "            \"score\": \"62\", \n",
      "            \"sort-name\": \"Kelly, Nirvana\", \n",
      "            \"type\": \"Person\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\", \n",
      "                \"name\": \"Gatineau\", \n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\", \n",
      "                \"name\": \"Gatineau\", \n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            }, \n",
      "            \"id\": \"02c4e6bb-7b7a-4686-8c23-df01bfd42b0e\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2012-04-05\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Sappy Nirvana Tribute\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Sappy Nirvana Tribute\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\", \n",
      "                \"name\": \"Graz\", \n",
      "                \"sort-name\": \"Graz\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\", \n",
      "                \"name\": \"Graz\", \n",
      "                \"sort-name\": \"Graz\"\n",
      "            }, \n",
      "            \"disambiguation\": \"Nirvana-Coverband\", \n",
      "            \"id\": \"46d8dae4-abec-438b-9c62-a3dbb2aaa1b7\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2000\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Teen Spirit\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Nirvana Teen Spirit\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"bb94730d-22c2-422d-a0a7-fe16a5b3e429\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Attainment of Nirvana\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"Attainment of Nirvana, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"c920948b-83e3-40b7-8fe9-9ab5abaac55b\", \n",
      "                \"name\": \"Houston\", \n",
      "                \"sort-name\": \"Houston\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"c920948b-83e3-40b7-8fe9-9ab5abaac55b\", \n",
      "                \"name\": \"Houston\", \n",
      "                \"sort-name\": \"Houston\"\n",
      "            }, \n",
      "            \"disambiguation\": \"Nirvana Cover Band\", \n",
      "            \"id\": \"45eacd92-6857-4faa-9283-023e72a1d4b1\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2012\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Nirvana Experience\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"The Nirvana Experience\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"e43ad11b-5d29-45ae-90f7-73ac47fb815d\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"smells like nirvana\", \n",
      "            \"score\": \"50\", \n",
      "            \"sort-name\": \"smells like nirvana\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"e1388435-f80d-434a-9980-f1c9f5aa9b90\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Nirvana Sitar & String Group\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Nirvana Sitar & String Group\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 19, \n",
      "    \"created\": \"2017-12-07T07:41:26.809Z\", \n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"area\": {\n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "    }, \n",
      "    \"begin-area\": {\n",
      "        \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "        \"name\": \"London\", \n",
      "        \"sort-name\": \"London\"\n",
      "    }, \n",
      "    \"country\": \"GB\", \n",
      "    \"disambiguation\": \"60s band from the UK\", \n",
      "    \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
      "    \"life-span\": {\n",
      "        \"begin\": \"1967\", \n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"Nirvana\", \n",
      "    \"score\": \"100\", \n",
      "    \"sort-name\": \"Nirvana\", \n",
      "    \"tags\": [\n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"progressive rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"orchestral\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"british\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"power pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"psychedelic rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"soft rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"symphonic rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"english\"\n",
      "        }\n",
      "    ], \n",
      "    \"type\": \"Group\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6?fmt=json&inc=releases\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": null, \n",
      "  \"country\": \"GB\", \n",
      "  \"date\": \"1969\", \n",
      "  \"disambiguation\": \"\", \n",
      "  \"id\": \"0b44cb36-550a-491d-bfd9-8751271f9de7\", \n",
      "  \"packaging\": null, \n",
      "  \"packaging-id\": null, \n",
      "  \"quality\": \"normal\", \n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\", \n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"GB\"\n",
      "        ], \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "      }, \n",
      "      \"date\": \"1969\"\n",
      "    }\n",
      "  ], \n",
      "  \"status\": \"Official\", \n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\", \n",
      "  \"text-representation\": {\n",
      "    \"language\": \"eng\", \n",
      "    \"script\": \"Latn\"\n",
      "  }, \n",
      "  \"title\": \"To Markos III\"\n",
      "}\n",
      "\n",
      "ALL TITLES:\n",
      "To Markos III\n",
      "Travelling on a Cloud\n",
      "Songs Of Love And Praise\n",
      "Songs of Love and Praise\n",
      "Songs of Love and Praise\n",
      "All of Us\n",
      "Secret Theatre\n",
      "The Story of Simon Simopath\n",
      "Me And My Friend\n",
      "All of Us\n",
      "The Story of Simon Simopath\n",
      "To Markos III\n",
      "Chemistry\n",
      "The Story of Simon Simopath\n",
      "Local Anaesthetic\n",
      "Orange & Blue\n",
      "Pentecost Hotel\n",
      "Black Flower\n",
      "All of Us\n",
      "Local Anaesthetic\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To experiment with this code freely you will have to run this code locally.\n",
    "Take a look at the main() function for an example of how to use the code. We\n",
    "have provided example json output in the other code editor tabs for you to look\n",
    "at, but you will not be able to run any queries through our UI.\n",
    "\"\"\"\n",
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    \"\"\"\n",
    "    This is the main function for making queries to the musicbrainz API. The\n",
    "    query should return a json document.\n",
    "    \"\"\"\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    \"\"\"\n",
    "    This adds an artist name to the query parameters before making an API call\n",
    "    to the function above.\n",
    "    \"\"\"\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    \"\"\"\n",
    "    After we get our output, we can use this function to format it to be more\n",
    "    readable.\n",
    "    \"\"\"\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Below is an example investigation to help you get started in your\n",
    "    exploration. Modify the function calls and indexing below to answer the\n",
    "    questions on the next quiz.\n",
    "\n",
    "    HINT: Note how the output we get from the site is a multi-level JSON\n",
    "    document, so try making print statements to step through the structure one\n",
    "    level at a time or copy the output to a separate output file. Experimenting\n",
    "    and iteration will be key to understand the structure of the data!\n",
    "    \"\"\"\n",
    "\n",
    "    # Query for information in the database about bands named Nirvana\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    # Isolate information from the 4th band returned (index 3)\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][3])\n",
    "\n",
    "    # Query for releases from that band using the artist_id\n",
    "    artist_id = results[\"artists\"][3][\"id\"]\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "\n",
    "    # Print information about releases from the selected band\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Quiz: Explore JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Problem set: Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Quiz: using csv module\n",
    "\n",
    "useful tips: \n",
    "* 1. csv.reader(file) --> list\n",
    "* 2. file.DictReader() --> Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rb') as f:\n",
    "        csv_file = csv.reader(f)\n",
    "        \n",
    "        for i,value in enumerate(csv_file):\n",
    "            if i == 0:\n",
    "                name = value[0].split(',')[1]\n",
    "            if i not in (0,1):\n",
    "                data.append(value)\n",
    "        \n",
    "    \n",
    "    # Do not change the line below\n",
    "    return (name, data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"1:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quiz: Excel to CSV\n",
    "\n",
    "note:\n",
    "\n",
    "* w = csv.writer(file,mode)\n",
    "* w.writerow(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import pprint\n",
    "\n",
    "DATADIR = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/\"\n",
    "filename = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "datafile = os.path.join(DATADIR,filename)\n",
    "outf = \"2013_Max_Loads.csv\"\n",
    "outfile = os.path.join(DATADIR,outf)\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for col_i in range(1,sheet.ncols - 1):\n",
    "        cv = sheet.col_values(col_i,start_rowx=1,end_rowx=None)\n",
    "        \n",
    "        maxval = max(cv)\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        maxtime = sheet.cell_value(maxpos,0)\n",
    "        max_real_time = xlrd.xldate_as_tuple(maxtime,0)\n",
    "        \n",
    "        data[sheet.cell_value(0,col_i)] = {\"maxval\": maxval,\n",
    "                                           \"maxtime\": max_real_time\n",
    "                                          }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    \n",
    "    with open(filename,'w') as f:\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "\n",
    "        for k,v in data.iteritems():\n",
    "            year, month, day, hour, _ , _= v[\"maxtime\"]\n",
    "            w.writerow([k,year, month, day, hour,v[\"maxval\"]]) \n",
    "    \n",
    "    \n",
    "def test():\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wrangling Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file \n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test() \n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "\n",
    "    for article in data:\n",
    "        section = article[\"section\"]\n",
    "        title = article[\"title\"]\n",
    "        titles.append({section: title})\n",
    "        if \"media\" in article:\n",
    "            for m in article[\"media\"]:\n",
    "                for mm in m[\"media-metadata\"]:\n",
    "                    if mm[\"format\"] == \"Standard Thumbnail\":\n",
    "                        urls.append(mm[\"url\"])\n",
    "\n",
    "    return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Data in more complex format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Quiz:  Exracting data\n",
    "\n",
    "Note:\n",
    "\n",
    "* find\n",
    "* findall\n",
    "* obj.find(tag).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None\n",
    "        }\n",
    "        \n",
    "        data[\"fnm\"] = author.find('fnm').text\n",
    "        data[\"snm\"] = author.find('snm').text\n",
    "        data[\"email\"] = author.find('email').text\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Quiz: handling attributes\n",
    "\n",
    "Note:  \n",
    "for item in insr:  \n",
    "...data[\"insr\"].append(item.attrib[\"iid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "        \n",
    "        insr = author.findall('./insr')\n",
    "        \n",
    "        for item in insr:\n",
    "            data[\"insr\"].append(item.attrib[\"iid\"])\n",
    "        \n",
    "        authors.append(data)\n",
    "        \n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "    \n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从网站抓取数据进行分析\n",
    "\n",
    "data wrangling procedule\n",
    "(For this exmple)\n",
    "\n",
    "- Build List of carrier values\n",
    "- Build List of airport values\n",
    "- Make HTTP request to download all data\n",
    "- Then parse the data files\n",
    "\n",
    "Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种: Tag , NavigableString , BeautifulSoup , Comment .\n",
    "\n",
    "Tag：\n",
    "Tag有很多方法和属性,在 遍历文档树 和 搜索文档树 中有详细解释.现在介绍一下tag中最重要的属性: name和attributes\n",
    "\n",
    "tag.name\n",
    "tag.attrs\n",
    "\n",
    "....\n",
    "\n",
    "遍历文档树\n",
    "tag的名字\n",
    "\n",
    "操作文档树最简单的方法就是告诉它你想获取的tag的name.如果想获取 <head> 标签,只要用 soup.head :\n",
    "\n",
    "这是个获取tag的小窍门,可以在文档树的tag中多次调用这个方法.下面的代码可以获取<body>标签中的第一个b标签:\n",
    "\n",
    "soup.body.b\n",
    "\n",
    "通过点取属性的方式只能获得当前名字的第一个tag:\n",
    "\n",
    "soup.a\n",
    "\n",
    "如果想要得到所有的a标签,或是通过名字得到比一个tag更多的内容的时候,就需要用到 Searching the tree 中描述的方法,比如: find_all()\n",
    "\n",
    "soup.find_all('a')\n",
    "\n",
    ".contents 和 .children\n",
    "\n",
    "tag的 .contents 属性可以将tag的子节点以列表的方式输出:\n",
    "\n",
    "通过tag的 .children 生成器,可以对tag的子节点进行循环:\n",
    "\n",
    "for child in title_tag.children:\n",
    "    print(child)\n",
    "    \n",
    ".string\n",
    "\n",
    "如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点:\n",
    "\n",
    "title_tag.string\n",
    " u'The Dormouse's story'\n",
    "\n",
    "如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同:\n",
    "\n",
    "head_tag.contents\n",
    " [<title>The Dormouse's story</title>]\n",
    "\n",
    "head_tag.string\n",
    " u'The Dormouse's story'\n",
    "\n",
    "父节点\n",
    "\n",
    "兄节点\n",
    "\n",
    "...\n",
    "\n",
    "搜索文档树\n",
    "\n",
    "Beautiful Soup定义了很多搜索方法,这里着重介绍2个: find() 和 find_all() .其它方法的参数和用法类似,请读者举一反三.\n",
    "\n",
    "正则表达式\n",
    "\n",
    "如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示<body>和b标签都应该被找到:\n",
    "\n",
    "import re\n",
    "for tag in soup.find_all(re.compile(\"^b\")):\n",
    "    print(tag.name)\n",
    " body\n",
    " b\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.  Using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        ev = soup.find(id=\"__EVENTVALIDATION\")\n",
    "        \n",
    "        data[\"eventvalidation\"] = ev[\"value\"]\n",
    "\n",
    "        vs = soup.find(id=\"__VIEWSTATE\")\n",
    "        data[\"viewstate\"] = vs[\"value\"]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "  \n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "r = requests.get(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "\n",
    "soup = BeautifulSoup(r.text)\n",
    "viewdate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewdate_element[\"value\"]\n",
    "\n",
    "eventvalidation_element = soup.find(id=\"__EVENTVALIDATION\")\n",
    "eventvalidation = eventvalidation_element[\"value\"]\n",
    "\n",
    "r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data=(('AirportList', \"BOS\"),\n",
    "                          ('CarrierList', \"VX\"),\n",
    "                          ('Submit', 'Submit'),\n",
    "                          (\"__EVENTTARGET\", \"\"),\n",
    "                          (\"__EVENTARGUMENT\", \"\"),\n",
    "                          (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                          (\"__VIEWSTATE\", viewstate\n",
    "                    )))\n",
    "\n",
    "f = open(\"virgin_and_logan_airport.html\",'w')\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Problem set: Data in more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Quiz: Carrier list\n",
    "\n",
    "Good:   \n",
    "print(\"\\n\".join(\"{} {}\".format(el['value'], el.get_text()) for el in airport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        airport = soup.find_all(\"option\")\n",
    "      \n",
    "        for el in airport:\n",
    "            if len(el[\"value\"]) == 2 :\n",
    "                data.append(el[\"value\"])\n",
    " \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"/Users/Simon/Desktop/ThinkPad410/DBS/Tasks/Study/Udacity/进阶/数据提取基础/options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        airport = soup.find_all(\"option\")\n",
    "      \n",
    "        for el in airport:\n",
    "            if len(el[\"value\"]) == 3 and el[\"value\"] != \"All\":\n",
    "                data.append(el[\"value\"])\n",
    " \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Data quality\n",
    "\n",
    "- 审计有效性 （validity）\n",
    "- 审计准确性  (accuracy)\n",
    "- 审计完整性  (completeness)\n",
    "- 审计一致性  (consistency) - 实际上是相信哪一个数据来源的问题\n",
    "- 审计统一性  (uniformity)  - 如统一字段中使用相同单位\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLUEPOINT FOR CLEANING**\n",
    "\n",
    "- Audit your data\n",
    "- Create a data cleaning plan\n",
    "    - Identify causes\n",
    "    - Define operations\n",
    "    - Test\n",
    "- Execute the plan (run script)\n",
    "- Manually correct\n",
    "\n",
    "***通常，对以上数据进行迭代，通常是2个以上的迭代，然后得到正确的结果***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audit Validity**\n",
    "\n",
    "e.g.\n",
    "- Data Type\n",
    "- Range\n",
    "- Regular expression\n",
    "- Cross-field constraints\n",
    "- Foreign Key constraints\n",
    "- MANDATORY\n",
    "- UNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1425: 1\n",
      "#1850: 1\n",
      "#500: 1\n",
      "#510: 1\n",
      "#575: 1\n",
      "2105: 1\n",
      "400: 1\n",
      "60637: 1\n",
      "Access: 1\n",
      "Ave: 2\n",
      "Avenu: 1\n",
      "Avenue: 66638\n",
      "Avenue,: 1\n",
      "Blvd: 2\n",
      "Bouevard: 1\n",
      "Boulevard: 4757\n",
      "Clinton: 1\n",
      "Court: 500\n",
      "Dr: 1\n",
      "Drive: 733\n",
      "Lane: 10\n",
      "LaSalle: 1\n",
      "level): 1\n",
      "Market: 52\n",
      "Monroe: 1\n",
      "NW: 1\n",
      "Park: 42\n",
      "Parkway: 602\n",
      "Place: 9343\n",
      "Plaza: 23\n",
      "Road: 1999\n",
      "South: 2\n",
      "St: 10\n",
      "St.: 2\n",
      "Street: 47532\n",
      "Terrace: 50\n",
      "Walk: 9\n",
      "Way: 2\n",
      "Whipple: 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "osm_file = open(\"chicagociitymap.osm\", \"r\")\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v) \n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])    \n",
    "    print_sorted_dict(street_types)    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 12. Quiz: Correcting validity\n",
    "\n",
    "Note the find() function   \n",
    "if row['URI'].find(\"dbpedia.org\") < 0:  \n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above, \n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad).\n",
    "\"\"\"\n",
    "import csv\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        \n",
    "        good_data = []\n",
    "        bad_data = []\n",
    "        \n",
    "        low_year = '1886'\n",
    "        high_year = '2014'\n",
    "        \n",
    "        def is_valid_year(year):\n",
    "            return low_year < year < high_year and year != \"NULL\"\n",
    "            \n",
    "        for i, line in enumerate(reader):                       \n",
    "            if i > 2:\n",
    "                productionEndYear = line[\"productionEndYear\"].split(\"-\")[0] \n",
    "                uri = line[\"URI\"].split(\"/\")[2]\n",
    "                \n",
    "                if is_valid_year(productionEndYear) and uri == \"dbpedia.org\":\n",
    "                    good_data.append(line)    \n",
    "                else:\n",
    "                    bad_data.append(line)\n",
    "                    \n",
    "\n",
    "    with open(output_good, \"w\") as g:\n",
    "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in good_data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            \n",
    "    with open(output_bad, \"w\") as b:\n",
    "        writer = csv.DictWriter(b, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in bad_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Problem set: Data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Quiz: Auditing Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areaCode': [<type 'str'>],\n",
      " 'areaLand': [<type 'str'>],\n",
      " 'areaMetro': [<type 'str'>],\n",
      " 'areaUrban': [<type 'str'>],\n",
      " 'elevation': [<type 'str'>],\n",
      " 'governmentType_label': [<type 'str'>],\n",
      " 'homepage': [<type 'str'>],\n",
      " 'isPartOf_label': [<type 'str'>],\n",
      " 'maximumElevation': [<type 'str'>],\n",
      " 'minimumElevation': [<type 'str'>],\n",
      " 'name': [<type 'str'>],\n",
      " 'populationDensity': [<type 'str'>],\n",
      " 'populationTotal': [<type 'str'>],\n",
      " 'timeZone_label': [<type 'str'>],\n",
      " 'utcOffset': [<type 'str'>],\n",
      " 'wgs84_pos#lat': [<type 'str'>],\n",
      " 'wgs84_pos#long': [<type 'str'>]}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    list_type = []\n",
    "    \n",
    "    with open(filename,'r') as f:\n",
    "        cv = csv.DictReader(f)\n",
    "        \n",
    "        for i,value in enumerate(cv):\n",
    "            if 1 <= i <= 2:\n",
    "                continue\n",
    "            \n",
    "         \n",
    "            for field in fields:\n",
    "                tp = type(value[field])\n",
    "                \n",
    "                if field in fieldtypes:\n",
    "                    if tp not in fieldtypes[field]:\n",
    "                        fieldtypes[field].append(tp)\n",
    "                else:\n",
    "                    fieldtypes[field] = list()\n",
    "                    fieldtypes[field].append(tp)\n",
    "        \n",
    "\n",
    "    return fieldtypes\n",
    "\n",
    "\n",
    "def test():\n",
    "    fieldtypes = audit_file(CITIES, FIELDS)\n",
    "\n",
    "    pprint.pprint(fieldtypes)\n",
    "\n",
    "    #assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "    #assert fieldtypes['areaMetro'] == set([type(1.1), type(None)])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quiz: Fix the area\n",
    "\n",
    "Note:\n",
    "\n",
    "reader.next()\n",
    "\n",
    "SIGNIFICANT DIGIT  - 有效数字\n",
    "\n",
    "operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Printing three example results:\n",
      "350166000.0\n",
      "41698800.0\n",
      "21496900.0\n",
      "6733970.0\n",
      "23050900.0\n",
      "13985900.0\n",
      "25122900.0\n",
      "28489900.0\n",
      "52576800.0\n",
      "20978900.0\n",
      "23309900.0\n",
      "30561900.0\n",
      "80289600.0\n",
      "224293000.0\n",
      "34187800.0\n",
      "23309900.0\n",
      "101787000.0\n",
      "31597900.0\n",
      "55166700.0\n",
      "63713700.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "import operator\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "\n",
    "def fix_area(area):\n",
    "    \n",
    "    if area == \"NULL\":\n",
    "        area = None\n",
    "    else:\n",
    "        list_area = area.split(\"|\")\n",
    "        dict_x = {}\n",
    "        \n",
    "        for x in list_area:\n",
    "            x = x.replace('{','')\n",
    "            x = x.replace('}','')\n",
    "            n = x.replace('.','')\n",
    "            \n",
    "            num = float(x)            \n",
    "            count = len(n.strip(\"0\"))\n",
    "                \n",
    "            dict_x[num] = count \n",
    "        \n",
    "            area = max(dict_x.iteritems(), key=operator.itemgetter(1))[0]\n",
    "                \n",
    "    return area\n",
    "\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "    print data[8][\"areaLand\"]\n",
    "    print \"Printing three example results:\"\n",
    "    for n in range(880,900):\n",
    "        pprint.pprint(data[n][\"areaLand\"])  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
